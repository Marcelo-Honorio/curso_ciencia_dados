---
title: "An√°lise de Agrupamento"
author: "Marcelo Honorio"
format: 
  revealjs:
    output-file: clustering.html
    theme: custom.scss
    transition: fade
    background-transition: fade
    highlight-style: ayu-mirage
    slide-number: c/t
    show-slide-number: all
editor: 
  markdown: 
    wrap: 72
---

## Supervisionado e N√£o Supervisionado

```{r echo=FALSE, out.width="40%"}
#| fig-align: "center"
knitr::include_graphics("imagens/sup-uns-learning2.jpg")
```

## Supervisionado e N√£o Supervisionado

```{r echo=FALSE, out.width="40%"}
#| fig-align: "center"
knitr::include_graphics("imagens/sup-uns-learning.jpg")
```

## M√©todo de Agrupamento

```{r echo=FALSE}
library(tidyverse)
library(dados)
library(kableExtra)
aluno <- readxl::read_excel('alunos_pap.xlsx')
```

<br>

O objetivo da an√°lise de cluster √© agrupar as observa√ß√µes em grupos de
tal forma que dentro de cada grupo as observa√ß√µes s√£o semelhantes e
distintas entre os grupos. <br>

Dentro de cada grupo a variabilidade deve ser m√≠nima e a variabilidade
entre os grupos deve ser m√°xima.

## Similaridade e Dissimilaridade

<br>

Dois objetos s√£o pr√≥ximos quando a sua dissemelhan√ßa ou dist√¢ncia √©
pequena ou a semelhan√ßa √© grande.

-   **Semelhan√ßa**: Quando queremos agrupar dados qualitativos.

```{r echo=FALSE, out.width="5%"}
#| fig-align: "center"
knitr::include_graphics("imagens/art_similaridade.png")
```

## Similaridade e Dissimilaridade

<br>

Dois objetos s√£o pr√≥ximos quando a sua dissemelhan√ßa ou dist√¢ncia √©
pequena ou a semelhan√ßa √© grande.

-   **Dissemelhan√ßa**: Quando queremos agrupar dados quantitativos

```{r echo=FALSE, out.width="5%"}
#| fig-align: "center"
knitr::include_graphics("imagens/art_dissemelhanca.png")
```

## Agrupar observa√ß√µes quantitativas
<br>
Vamos supor que queremos formar grupos de alunos:

```{r, echo=FALSE}
#| fig-align: center
knitr::kable(aluno, col.names = gsub("[.]", " ", names(aluno)), align = "lccrr") |> 
  kableExtra::kable_styling(bootstrap_options = c('striped', 'condensed', 'houver'), full_width = F, row_label_position = "c", font_size = 25)
```

. . .

-   Quantos grupos?

-   Como vamos determinar os grupos?




## Agrupar observa√ß√µes quantitativas

::::{.columns}
::: {.column width="30%"}
<br>
```{r, echo=FALSE}
knitr::kable(aluno, col.names = gsub("[.]", " ", names(aluno)), align = "lccrr") |> 
  kableExtra::kable_styling(bootstrap_options = c('striped', 'condensed', 'houver'), full_width = F, row_label_position = "c", font_size = 20)
```
:::

::: {.column width="70%"}
```{r echo=FALSE}
#| fig-align: center
ggplot(aluno) + 
  geom_point(aes(x = Matematica, y = Portugues), size = 4) + 
  geom_label(aes(x = Matematica, y = Portugues, label = Aluno), 
             fill= c('white', 'white', 'white', 'white', 'white', 'white')) +
  scale_x_continuous(limits = c(0, 10)) +
  scale_y_continuous(limits = c(0, 10)) +
  theme_classic()
```
:::
::::

## Agrupar observa√ß√µes quantitativas

::::{.columns}
::: {.column width="30%"}
<br>
```{r, echo=FALSE}
knitr::kable(aluno, col.names = gsub("[.]", " ", names(aluno)), align = "lccrr") |> 
  kableExtra::kable_styling(bootstrap_options = c('striped', 'condensed', 'houver'), full_width = F, row_label_position = "c", font_size = 20)
```
:::

::: {.column width="70%"}
```{r echo=FALSE}
#| fig-align: center
ggplot(aluno) + 
  geom_point(aes(x = Matematica, y = Portugues), size = 4) + 
  geom_label(aes(x = Matematica, y = Portugues, label = Aluno), 
             fill= c('white', 'white', 'white', 'white', 'white', 'white')) +
  geom_line(aes(x = c(NA, NA, 6, NA, 4, NA), y = c(NA, NA, 6, NA, 4, NA)), color = 'red', linetype = 2, linewidth = 1.2) +
  geom_text(aes(x = Matematica, y = Portugues), 
            label=c('', '?', '', '', '', ''), 
            size = 10 ,hjust = 2, vjust = -2, color="red") +
  scale_x_continuous(limits = c(0, 10)) +
  scale_y_continuous(limits = c(0, 10)) +
  theme_classic()
```
:::
::::


Como podemos medir a dissemelhen√ßa ou dist√¢ncia entre os alunos?

## Medidas de dissemelhan√ßa

<br>

Os coeficientes de dissemelhan√ßa para dados quantitativos mais
utilizados s√£o:

```{r echo=FALSE, out.width="10%"}
#| fig-align: "center"
#| fig-width: 10
#| fig-height: 6
knitr::include_graphics("imagens/dissemelhanca1.png")
```

## Medidas de dissemelhan√ßa

<br>

Os coeficientes de dissemelhan√ßa para dados quantitativos mais
utilizados s√£o:

```{r echo=FALSE, out.width="10%"}
#| fig-align: "center"
#| fig-width: 10
#| fig-height: 6
knitr::include_graphics("imagens/dissemelhanca2.png")
```

## Medidas de dist√¢ncia

-   Dist√¢ncia Euclidiana:
    $$\displaystyle D_E ={||x -  w||=\sqrt{\sum_{i=1}^{k} (x_i - w_i)^2}}$$

-   A dist√¢ncia Euclidiana gera a dist√¢ncia linear entre quaisquer dois
    pontos em um campo com k dimens√µes.

-   √â uma generaliza√ß√£o do Teorema de Pit√°goras

## Medidas de dist√¢ncia

-   Dist√¢ncia Minkowsky:
    $$\displaystyle D_p (x_i, x_j) = \left({\sum_{k=1}^{d} |x_{ik} - x_{jk}|^p} \right)^{\frac{1}{p}}$$

-   Pode criar pesos para cada vari√°vel, quando necess√°rio.

-   √â uma medida menos afetada pela presen√ßa de outliers (comparada a
    dist√¢ncia euclidiana).

# Dist√¢ncia Euclidiana

## Dist√¢ncia entre **B** e **D**

Agrupar alunos que sejam parecidos, com rela√ß√£o as notas em matem√°tica e
portug√™s.

::: columns
::: {.column width="40%"}
<br>

```{r echo=FALSE}
#| fig-align: "center"
knitr::kable(aluno, col.names = gsub("[.]", " ", names(aluno)), align = "lccrr") |> 
  kableExtra::kable_styling(bootstrap_options = c('striped', 'condensed', 'houver'), full_width = F, row_label_position = "c", font_size = 25)
```
:::

::: {.column width="60%"}
<br>

```{r echo=FALSE, out.width="80%"}
#| fig-align: "center"
knitr::include_graphics("imagens/distecludiana.jpg")
```
:::
:::

## Dist√¢ncia entre **B** e **D**

Agrupar alunos que sejam parecidos, com rela√ß√£o as notas em matem√°tica e
portug√™s.

::: columns
::: {.column width="40%"}
<br>

```{r echo=FALSE}
#| fig-align: "center"
knitr::kable(aluno, col.names = gsub("[.]", " ", names(aluno)), align = "lccrr") |> 
  kableExtra::kable_styling(bootstrap_options = c('striped', 'condensed', 'houver'), full_width = F, row_label_position = "c", font_size = 25)
```
:::

::: {.column width="60%"}
$$\displaystyle D^2 ={(x_{14} - x_{12} )^2 + (x_{24} - x_{22})^2}$$
$$\displaystyle D^2 ={(5 - 10 )^2 + (4 - 8)^2}$$
$$\displaystyle D^2 ={5^2 + 4^2 = 41}$$

$$\displaystyle D ={\sqrt{41}} = 6,40$$
:::
:::

## Matriz de Dist√¢ncia

A matriz $D_0$ composta pelas dist√¢ncias euclidianas entre cada par de
observa√ß√µes, conforme segue:

```{r}
d <- as.matrix(dist(aluno[2:3]))
d <- data.frame(d, row.names = LETTERS[1:6])
colnames(d) <- LETTERS[1:6]

knitr::kable(round(d[2:6, 1:5], 4), align = "lccrr", booktabs = T, escape = F) |> 
  kable_styling("striped", full_width = F)  |> 
  row_spec(0, font_size = 25, monospace = F)  |> 
  row_spec(1:5, font_size = 25, color = 'black') |> 
  column_spec(1, bold = T)
```

<br>

Inicialmente cada observa√ß√£o √© considerada um cluster individual, ou
seja, no est√°gio 0, temos 6 clusters.

## Matriz de Dist√¢ncia

A matriz $D_0$ composta pelas dist√¢ncias euclidianas entre cada par de
observa√ß√µes, conforme segue:

```{r}
d <- as.matrix(dist(aluno[2:3]))
d <- data.frame(d, row.names = LETTERS[1:6])
colnames(d) <- LETTERS[1:6]

knitr::kable(round(d[2:6, 1:5], 4), align = "lccrr", booktabs = T, escape = F) |> 
  kable_styling("striped", full_width = F)  |> 
  row_spec(0, font_size = 25, monospace = F)  |> 
  row_spec(1:5, font_size = 25, color = 'black') |> 
  column_spec(1, bold = T)
```

<br>

Depois de calcular as dist√¢ncias quem pode formar grupo?


## M√©todos de agrupamento

<br>

::: columns
::: column
**Hier√°rquico**

```{r echo=FALSE}
data <- matrix( sample(seq(1,2000),200), ncol = 10 )
rownames(data) <- paste0("sample_" , seq(1,20))
colnames(data) <- paste0("variable",seq(1,10))

# Euclidean distance
dist <- dist(data[ , c(4:8)] , diag=TRUE)

# Hierarchical Clustering with hclust
hc <- hclust(dist)

# Plot the result
plot(hc)
```
:::

::: column
**N√£o Hier√°rquico**

```{r echo=FALSE, out.width="80%"}
#| fig-align: "center"
knitr::include_graphics("imagens/k-means.png")
```
:::
:::

## M√©todos de agrupamento

<br>

::: columns
:::{.column width="40%"}
**Hier√°rquico**

```{r echo=FALSE}
data <- matrix( sample(seq(1,2000),200), ncol = 10 )
rownames(data) <- paste0("sample_" , seq(1,20))
colnames(data) <- paste0("variable",seq(1,10))

# Euclidean distance
dist <- dist(data[ , c(4:8)] , diag=TRUE)

# Hierarchical Clustering with hclust
hc <- hclust(dist)

# Plot the result
plot(hc)
```
:::

::: column
-   **positivo**: conseguimos acompanhar a forma√ß√£o dos grupos.
-   **negativo**: √© menos eficiente em grande volume de dados.
:::
:::

## M√©todos de agrupamento

<br>

::: columns
::: {.column width="40%"}
**N√£o Hier√°rquico**

```{r echo=FALSE, out.width="100%"}
#| fig-align: "center"
knitr::include_graphics("imagens/k-means.png")
```
:::

::: column
<br>

-   **positivo**: trabalha bem com grande volume de dados.
-   **negativo**: a quantidade de cluster deve ser decidida antes.
:::
:::

# Hier√°rquico

## Primeiro est√°gio

A menor dist√¢ncia entre todas as observa√ß√µes e, portanto, ser√£o
inicialmente agrupadas formando um novo *cluster*.

```{r echo=FALSE}
#| fig-align: center
ggplot(aluno) + 
  geom_point(aes(x = Matematica, y = Portugues)) + 
  geom_label(aes(x = Matematica, y = Portugues, label = Aluno), label.size = .6,
             fill= c('white', 'red', 'white', 'white', 'red', 'white')) +
  geom_line(aes(x = c(NA, 5, NA, NA, 4, NA), y = c(NA, 4, NA, NA, 4, NA)), color = 'red', linetype = 2, linewidth = 1.2) +
  scale_x_continuous(limits = c(0, 10)) +
  scale_y_continuous(limits = c(0, 10)) +
  labs(x = 'Matem√°tica', y = 'Portugu√™s') +
  theme_classic()
  
```

. . .

Como vamos calcular a dist√¢ncia entre o *cluster* [**B-E**]{style="color:red"} e as demais observa√ß√µes?

## T√©cnicas de Agrupamentos

<br>

A escolha do m√©todo de aglomera√ß√£o √© t√£o importante quanto a defini√ß√£o
da medida de dist√¢ncia.

-   **Single Linkage** - Vizinho mais pr√≥ximo
-   **Complete Linkage** - Vizinho mais longe
-   **Avarage Linkage** - M√©dia
-   **Centroid M√©thod** - Centro√≠de

## T√©cnicas de Agrupamentos

A escolha do m√©todo de aglomera√ß√£o √© t√£o importante quanto a defini√ß√£o
da medida de dist√¢ncia.

```{r echo=FALSE, out.width="100%"}
#| fig-align: "center"
knitr::include_graphics("imagens/met_encadeamento.png")
```

## T√©cnicas de Agrupamentos

Calculadas as dist√¢ncias entre o cluster [(**B-E**)]{style="color:red"} e os demais alunos.

```{r}
knitr::kable(round(d[2:6, 1:5], 4), align = "lccrr", booktabs = T, escape = F) |> 
  kable_styling("striped", full_width = F)  |> 
  row_spec(0, font_size = 20, monospace = F)  |> 
  row_spec(1:5, font_size = 20, color = 'black') |> 
  column_spec(1, bold = T)
```

. . .

A dist√¢ncia de (**B-E**) para C √© 2.2360 e 2.8284:

. . .

- single: 2.2360
- complete: 2.8284

## T√©cnicas de Agrupamentos

```{r echo=FALSE, out.width="70%"}
#| fig-align: "center"
knitr::include_graphics("imagens/tec_agrupamento.png")
```


## Segundo est√°gio

Portando, no segundo est√°gio, um segundo cluster ser√° formado [(**A-D**)]{style="color:green"}

```{r echo=FALSE}
#| fig-align: center
ggplot(aluno) + 
  geom_point(aes(x = Matematica, y = Portugues)) + 
  geom_label(aes(x = Matematica, y = Portugues, label = Aluno), label.size = .6,
             fill= c('green', 'red', 'white', 'green', 'red', 'white')) +
  geom_line(aes(x = c(NA, 5, NA, NA, 4, NA), y = c(NA, 4, NA, NA, 4, NA)), color = 'red', linetype = 2, linewidth = 1.2) +
  geom_line(aes(x = c(9, NA, NA, 10, NA, NA), y = c(7, NA, NA, 8, NA, NA)), color = 'green', linetype = 2, linewidth = 1.2) +
  scale_x_continuous(limits = c(0, 10)) +
  scale_y_continuous(limits = c(0, 10)) +
  labs(x = 'Matem√°tica', y = 'Portugu√™s') +
  theme_classic()
  
```

Os alunos **C** e **F** permanecem ainda isolados

## Terceiro est√°gio

Neste est√°gio C √© inserido no cluster j√° formado [(**E-B**)]{style="color:red"}

```{r echo=FALSE}
#| fig-align: center
ggplot(aluno) + 
  geom_point(aes(x = Matematica, y = Portugues)) + 
  geom_label(aes(x = Matematica, y = Portugues, label = Aluno), label.size = .6,
             fill= c('green', 'red', 'red', 'green', 'red', 'white')) +
  geom_line(aes(x = c(NA, 5, 6, NA, 4, NA), y = c(NA, 4, 6, NA, 4, NA)), color = 'red', linetype = 2, linewidth = 1.2) +
  geom_line(aes(x = c(9, NA, NA, 10, NA, NA), y = c(7, NA, NA, 8, NA, NA)), color = 'green', linetype = 2, linewidth = 1.2) +
  scale_x_continuous(limits = c(0, 10)) +
  scale_y_continuous(limits = c(0, 10)) +
  labs(x = 'Matem√°tica', y = 'Portugu√™s') +
  theme_classic()
  
```

O aluno **F** permanece ainda isolado

## Dendrograma

Como base nesse esquema de aglomera√ß√£o, podemos construir um gr√°fico em
formato de √°rvore, conhecido como **dendrograma**

```{r echo=FALSE}
#| fig-align: center
dig <- aluno |> 
  remove_rownames() |> 
  column_to_rownames(var="Aluno") |> 
  dist() |> 
  hclust() |> 
  as.dendrogram() 

dig |> 
  plot(main = "M√©todo Complete")
```

## Dendrograma

<br>

::: columns
::: column
**Single**

```{r echo=FALSE}
#| fig-align: center
dig <- aluno |> 
  remove_rownames() |> 
  column_to_rownames(var="Aluno") |> 
  dist() |> 
  hclust(method = "single") |> 
  as.dendrogram() 

dig |> 
  plot()
```
:::

::: column
**Complete**

```{r echo=FALSE}
#| fig-align: center
dig <- aluno |> 
  remove_rownames() |> 
  column_to_rownames(var="Aluno") |> 
  dist() |> 
  hclust(method = "complete") |> 
  as.dendrogram() 

dig |> 
  plot()
```
:::
:::

## Dendrograma

<br>

::: columns
::: column
**Average**

```{r echo=FALSE}
#| fig-align: center
dig <- aluno |> 
  remove_rownames() |> 
  column_to_rownames(var="Aluno") |> 
  dist() |> 
  hclust(method = "average") |> 
  as.dendrogram() 

dig |> 
  plot()
```
:::

::: column
**Centroid**

```{r echo=FALSE}
#| fig-align: center
dig <- aluno |> 
  remove_rownames() |> 
  column_to_rownames(var="Aluno") |> 
  dist() |> 
  hclust(method = "centroid") |> 
  as.dendrogram() 

dig |> 
  plot()
```
:::
:::

## N√∫mero de Cluster

Como voc√™ decidiria o n√∫mero de cluster?

```{r echo=FALSE}
#| fig-align: center
# Hierarchical Clustering with hclust
dig <- aluno |> 
  remove_rownames() |> 
  column_to_rownames(var="Aluno") |> 
  dist() |> 
  hclust() |> 
  as.dendrogram() 

dig |> 
  plot(xlab = c(1:6), main = "M√©todo Complete")
```

## N√∫mero de Cluster

Como voc√™ decidiria o n√∫mero de cluster?

```{r echo=FALSE}
#| fig-align: center
# Hierarchical Clustering with hclust
dig <- aluno |> 
  remove_rownames() |> 
  column_to_rownames(var="Aluno") |> 
  dist() |> 
  hclust() |> 
  as.dendrogram() 

dig |> 
  plot(xlab = c(1:6), main = "M√©todo Complete")

abline(h = 3, lty = 2, col = "red")
```

## N√∫mero de Cluster

3 grupos √© uma boa solu√ß√£o?
```{r echo=FALSE}
#| fig-align: center
ggplot(aluno) + 
  geom_point(aes(x = Matematica, y = Portugues)) + 
  geom_label(aes(x = Matematica, y = Portugues, label = Aluno), label.size = .6,
             fill= c('green', 'red', 'red', 'green', 'red', 'blue')) +
  geom_line(aes(x = c(NA, 5, 6, NA, 4, NA), y = c(NA, 4, 6, NA, 4, NA)), color = 'red', linetype = 2, linewidth = 1.2) +
  geom_line(aes(x = c(9, NA, NA, 10, NA, NA), y = c(7, NA, NA, 8, NA, NA)), color = 'green', linetype = 2, linewidth = 1.2) +
  scale_x_continuous(limits = c(0, 10)) +
  scale_y_continuous(limits = c(0, 10)) +
  labs(x = 'Matem√°tica', y = 'Portugu√™s') +
  theme_classic()
  
```


# T√©cnicas para escolher o n√∫mero de cluster

## M√©todo Elbow

Assim como no princ√≠pio de cluster, a ideia do m√©todo √© minimizar a
variabilidade dentro do cluster, ou seja:

$$\small min(\sum_{i=1}^{n} W(C_k))$$ onde $C_k$ √© o ùëò cluster e
$ùëä(C_k)$ √© a varia√ß√£o dentro do cluster.

Ent√£o, o total da soma dos quadrados dentro do cluster $(wss)$ mede a
homogeneidade do cluster e queremos que seja t√£o pequeno quanto poss√≠vel.

## M√©todo Elbow

De acordo com o m√©todo, o n√∫mero ideal de grupos se d√° quando o ponto
forma uma curva semelhante de um cotovelo, que √© o ponto considerado
ideal.

```{r echo=FALSE, out.width="100%"}
#| fig-align: "center"
knitr::include_graphics("imagens/metodo_cotovelo.png")
```

## M√©todo Silhueta

<br>

Determina o qu√£o bem cada objeto est√° alocado em um grupo, ou seja, a
homogeneidade de um grupo.

$$\small s_i = \frac {\bar x_i - \bar y_i}{max(\bar y_i, \bar x_i)}$$

$\bar y_i$ √© a dist√¢ncia m√©dia entre o ponto e todos os demais pontos do
cluster. $\bar x_i$ √© a dist√¢ncia m√©dia entre o ponto e todos os pontos
do cluster vizinho mais pr√≥ximo.

## M√©todo Silhueta

O √≠ndice de Silhueta varia de -1 a 1. Valores pr√≥ximos a 1 indicam que o
objeto possui semelhan√ßa com objetos de seu grupo e dessemelhan√ßa com
objetos de outros grupos.

```{r echo=FALSE, out.width="100%"}
#| fig-align: "center"
knitr::include_graphics("imagens/indice_silhueta.png")
```

## Padroniza√ß√£o

<br>

√â necess√°rio padronizar vari√°veis em unidades distintas. O m√©doto mais
comumente utilizado para padroniza√ß√£o de vari√°veis √© conhecido por
**procedimento Zscore**
$$\displaystyle Z_{score}(X)={\frac {x_{i} - \bar x}{DesvioPadr√£o}}$$

# N√£o Hier√°rquico

## N√£o Hier√°rquico

<br>

o Metodo hier√°rquico √© menos eficiente com grande volume de dados

```{r echo=FALSE}
#| fig-align: center
knitr::include_graphics("imagens/dendrogram_big.png")
```

## k-means

<br>

O m√©todo K-means classifica os objetos dentro de m√∫ltiplos grupos, de forma que a varia√ß√£o intra-cluster seja minimizada pela soma dos quadrados das dist√¢ncias Euclidianas entre os itens e seus centroides.

$$\small W(C_k) = \sum_{x_i \in C_k}^{n} (x_i - \bar x_k)^2$$


## K-means

O m√©todo k-means utiliza a dist√¢ncia Euclidiana como medida de dissimilaridade.

```{r echo=FALSE}
#| fig-align: center
knitr::include_graphics("imagens/plot-k-means.png")
```

## K-means

√â necess√°rio avisar de antem√£o o valor de k.

```{r echo=FALSE}
#| fig-align: center
knitr::include_graphics("imagens/plot-k-means1.png")
```

## K-means

O objetivo √© minimizar a soma dos quadrados intra-cluster

```{r echo=FALSE}
#| fig-align: center
knitr::include_graphics("imagens/plot-k-means2.png")
```

## K-means

O objetivo √© minimizar a soma dos quadrados intra-cluster

```{r echo=FALSE}
#| fig-align: center
knitr::include_graphics("imagens/plot-k-means3.png")
```

## K-means

<br>

A metodologia k-means segue 4 passos:

1.   Seleciona randomicamente os centr√≥ides iniciais (sementes).

2.   Calcula dist√¢ncia Euclidiana em rela√ß√£o aos seus centr√≥ides

3.   Atualiza os centr√≥ides, esse processo √© repetido at√© a converg√™ncia.

4.   Uma leitura final dos dados assinala cada observa√ß√£o ao centr√≥ide mais pr√≥ximo.
